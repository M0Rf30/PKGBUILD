# Configuration file for https://github.com/mudler/LocalAI

# Path containing models used for inferencing
LOCALAI_MODELS_PATH="/var/lib/local-ai/models"

# Path used to extract libraries required by some backends at runtime
LOCALAI_BACKEND_ASSETS_PATH="/var/lib/local-ai/backend_data"

# Location for images generated by backends (e.g., stablediffusion)
LOCALAI_IMAGE_PATH="/var/lib/local-ai/generated/images"

# Location for audio generated by backends (e.g., piper)
LOCALAI_AUDIO_PATH="/var/lib/local-ai/generated/audio"

# Path to store uploads from files API
# LOCALAI_UPLOAD_PATH="/tmp/localai/upload"

# Path for configuration files
# LOCALAI_CONFIG_PATH="/tmp/localai/config"

# Directory for dynamic loading of certain configuration files
LOCALAI_CONFIG_DIR="/etc/local-ai"

# Interval to poll the LocalAI Config Directory for changes
# LOCALAI_CONFIG_DIR_POLL_INTERVAL="1m"

# YAML file containing a list of model backend configs
# LOCALAI_MODELS_CONFIG_FILE="config.yaml"

# JSON list of 
# LOCALAI_GALLERIES='[{"name":"model-gallery", "url":"github:go-skynet/model-gallery/index.yaml"}, {"url": "github:go-skynet/model-gallery/huggingface.yaml","name":"huggingface"}]'

# Enable automatic loading of galleries
# LOCALAI_AUTOLOAD_GALLERIES=true

# A LocalAI remote library URL
# LOCALAI_REMOTE_LIBRARY="${remoteLibraryURL}"

# List of models to apply at startup in JSON format
# LOCALAI_PRELOAD_MODELS="[]"

# List of model configuration URLs to load
# LOCALAI_MODELS="[]"

# Path to a YAML config file for preloading models at startup
# LOCALAI_PRELOAD_MODELS_CONFIG="preload_config.yaml"

# Enable GPU acceleration
# LOCALAI_F16=true

# Number of threads used for parallel computation
# LOCALAI_THREADS=4

# Default context size for models
# LOCALAI_CONTEXT_SIZE=512

# Bind address for the API server
# LOCALAI_ADDRESS=":8080"

# Enable CORS for the API
LOCALAI_CORS=true

# Path to the library directory for external libraries used by backends
# LOCALAI_LIBRARY_PATH="/usr/share/local-ai/libs"

# Enable fiber CSRF middleware
# LOCALAI_CSRF=true

# Default upload limit in MB
# LOCALAI_UPLOAD_LIMIT=15

# List of API Keys for authentication
# LOCALAI_API_KEY="[]"

# Disable web UI
# LOCALAI_DISABLE_WEBUI=false

# Disable the best-effort security scanner before downloading files
# LOCALAI_DISABLE_PREDOWNLOAD_SCAN=false

# Replace all error responses with blank 500 errors
# LOCALAI_OPAQUE_ERRORS=false

# Use constant-time comparisons for API Key validation
# LOCALAI_SUBTLE_KEY_COMPARISON=false

# Disable API key requirement for HTTP GET requests
# LOCALAI_DISABLE_API_KEY_REQUIREMENT_FOR_HTTP_GET=false

# Disable the /metrics endpoint
# LOCALAI_DISABLE_METRICS_ENDPOINT=false

# List of endpoints exempt from API key requirement for GET requests
# LOCALAI_HTTP_GET_EXEMPTED_ENDPOINTS="^/$,^/browse/?$,^/talk/?$,^/p2p/?$,^/chat/?$,^/text2image/?$,^/tts/?$,^/static/.*$,^/swagger.*$"

# Enable P2P mode
# LOCALAI_P2P=false

# Interval for DHT refresh during token generation
# LOCALAI_P2P_DHT_INTERVAL=360

# Interval for OTP refresh during token generation
# LOCALAI_P2P_OTP_INTERVAL=9000

# Token for P2P mode (optional)
# LOCALAI_P2P_TOKEN=""

# Network ID for P2P mode
# LOCALAI_P2P_NETWORK_ID=""

# Enable backends to handle multiple requests in parallel
# LOCALAI_PARALLEL_REQUESTS=true

# Allow only one backend to be run at a time
# LOCALAI_SINGLE_ACTIVE_BACKEND=false

# Do not launch API services, only preloaded models/backends are started
# LOCALAI_PRELOAD_BACKEND_ONLY=false

# List of external gRPC backends
# LOCALAI_EXTERNAL_GRPC_BACKENDS="[]"

# Enable watchdog for stopping idle backends
# LOCALAI_WATCHDOG_IDLE=false

# Threshold for stopping idle backends
# LOCALAI_WATCHDOG_IDLE_TIMEOUT="15m"

# Enable watchdog for stopping busy backends
# LOCALAI_WATCHDOG_BUSY=false

# Threshold for stopping busy backends
# LOCALAI_WATCHDOG_BUSY_TIMEOUT="5m"

# Enable federated instance
# LOCALAI_FEDERATED=false

# Disable the gallery endpoints
# LOCALAI_DISABLE_GALLERY_ENDPOINT=false

# List of models to load into memory at startup
# LOCALAI_LOAD_TO_MEMORY="[]"
